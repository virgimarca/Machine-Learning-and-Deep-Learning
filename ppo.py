# -*- coding: utf-8 -*-
"""PPO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iwJhgTlf8M7qybNvX0WaskvBqXCKemLW
"""

!apt-get install -y \
    libgl1-mesa-dev \
    libgl1-mesa-glx \
    libglew-dev \
    libosmesa6-dev \
    software-properties-common

!apt-get install -y patchelf

!pip install gym
!pip install free-mujoco-py
!pip install stable_baselines3

!unzip classes.zip

import torch
import gym
import argparse

from classes.env.custom_hopper import *
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy

n_episodes = 100000
device = 'cpu'

env_source = gym.make('CustomHopper-source-v0')
env_target = gym.make('CustomHopper-target-v0')

"""
  Training
"""
model = PPO("MlpPolicy", env_source, learning_rate=1e-3, batch_size=2048, gamma=0.99, verbose = 0, device = device)
model.learn(total_timesteps= n_episodes*50, n_eval_episodes = n_episodes, eval_log_path = '/content/ppo')
model.save("ppo_hopper_lr1e.mdl")

"""
  Testing
"""

# Evaluate the trained agent
mean_reward, std_reward = evaluate_policy(model, env_source, n_eval_episodes=50)

print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")