# -*- coding: utf-8 -*-
"""test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LXmyh0VU4BnDKPm-LmECICOTqPMP3vSf

Install and load all dependencies (first time only) \
NOTE: you may need to restart the runtime afterwards (CTRL+M .).
"""

!apt-get install -y \
    libgl1-mesa-dev \
    libgl1-mesa-glx \
    libglew-dev \
    libosmesa6-dev \
    software-properties-common

!apt-get install -y patchelf

!pip install gym
!pip install free-mujoco-py

"""Set up the custom Hopper environment



1.   Upload `classes.zip` to the current session's file storage
2.   Un-zip it by running cell below

"""

!unzip classes.zip

from google.colab import drive
drive.mount('/content/drive')

"""---

\

**Test an RL agent on the OpenAI Gym Hopper environment**

\
Choose which agent you want to test.
"""

import torch
import gym

from classes.env.custom_hopper import *
#from classes.vanilla import Agent, Policy
from classes.actor_critic import Agent, Policy
import numpy as np
import matplotlib.pyplot as plt

model = 'model.mdl' # Fill in model path
device = 'cpu'
episodes = 50

#@title
#env = gym.make('CustomHopper-source-v0')
env = gym.make('CustomHopper-target-v0')

print('Action space:', env.action_space)
print('State space:', env.observation_space)
print('Dynamics parameters:', env.get_parameters())

observation_space_dim = env.observation_space.shape[-1]
action_space_dim = env.action_space.shape[-1]

policy = Policy(observation_space_dim, action_space_dim)
policy.load_state_dict(torch.load(model), strict=True)

agent = Agent(1e-4, 1, policy, device=device)

test_rewards = []
for episode in range(episodes):
  done = False
  test_reward = 0
  state = env.reset()

  while not done:
    action, _ = agent.get_action(state, evaluation=True)
    
    state, reward, done, info = env.step(action.detach().cpu().numpy())

    test_reward += reward
  test_rewards.append(test_reward)
  print(f"Episode: {episode} | Return: {test_reward}")

avg = np.sum(test_rewards)/len(test_rewards)
