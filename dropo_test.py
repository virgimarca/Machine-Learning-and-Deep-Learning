# -*- coding: utf-8 -*-
"""DROPO_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t2bgkdpxH7QYUWsaHx20kGx7Xzyb8gs3
"""

!apt-get install -y \
    libgl1-mesa-dev \
    libgl1-mesa-glx \
    libglew-dev \
    libosmesa6-dev \
    software-properties-common

!apt-get install -y patchelf

!pip install gym
!pip install free-mujoco-py
!pip install stable_baselines3

!unzip classes.zip

from classes.dropo import *
from classes.env.custom_hopper import *
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy

env = gym.make('CustomHopper-target-v0')
K = 100
lambda_t = 50
epsilon = 1e-3
instances = 200
dropo = Dropo(env, lambda_t, epsilon, K, instances)
budget = 100
res = dropo.optimize(budget)

n_episodes = 100000
device = 'cpu'
env_source = gym.make('CustomHopper-source-v0')
env_source.set_dropo_parameters(res)
env_source.set_distribution('truncnormal')

"""
  Training
"""
model = PPO("MlpPolicy", env_source, learning_rate=1e-3, gamma=0.99, verbose = 0, device = device)
model.learn(total_timesteps=n_episodes, n_eval_episodes = 50, eval_log_path = '/content/ppo')
model.save("ppo_hopper_dropo.mdl")

"""
  Testing
"""
#env = gym.make('CustomHopper-source-v0')
env = gym.make('CustomHopper-target-v0')
# Evaluate the trained agent
mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=50)

print(f"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}")
